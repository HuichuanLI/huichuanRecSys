{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', 2), ('spark', 5), ('hive', 1), ('sql', 2)]\n",
      "hadoop 2\n",
      "spark 5\n",
      "hive 1\n",
      "sql 2\n",
      "spark 5\n",
      "hadoop 2\n",
      "sql 2\n",
      "[('spark', 5), ('hadoop', 2), ('sql', 2)]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(\"/Users/hui/Desktop/bigdata/spark-2.4.4-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sql\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from collections import defaultdict\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf().setAppName('helloworld').setMaster('local').set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "    \n",
    "    #spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel('WARN')\n",
    "    \"\"\"\n",
    "        创建RDD:\n",
    "        方式一：\n",
    "        \n",
    "    \"\"\"\n",
    "    datas = [\"hadoop spark\", \"spark hive spark sql\", \"spark hadoop sql spark\"]\n",
    "    rdd = sc.parallelize(datas)\n",
    "    word_count = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "    print(word_count.collect())\n",
    "    for x in word_count.collect():\n",
    "        print(x[0],x[1])\n",
    "        \n",
    "    sort_rdd =word_count.map(lambda count:(count[1],count[0])).sortByKey(ascending=False)\n",
    "    \n",
    "    # 获取top\n",
    "    top_rdd = word_count.top(3,key=lambda x:x[1])\n",
    "    \n",
    "    for word,count in top_rdd:\n",
    "        print(word,count)\n",
    "    print(top_rdd)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_map():\n",
    "    data = [1,2,3,4,5]\n",
    "    rdd1 = sc.parallelize(data)\n",
    "    rdd2 = rdd1.map(lambda x:x*2)\n",
    "    print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"spark0401\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf = conf)\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "my_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def my_map2():\n",
    "        a = sc.parallelize([\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \" eagle\"])\n",
    "        b = a.map(lambda x:(x,1))\n",
    "        print(b.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 1), ('tiger', 1), ('lion', 1), ('cat', 1), ('panther', 1), (' eagle', 1)]\n"
     ]
    }
   ],
   "source": [
    "my_map2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter():\n",
    "    data = [1,2,3,4,5]\n",
    "    rdd1 = sc.parallelize(data)\n",
    "    mapRdd = rdd1.map(lambda x:x*2)\n",
    "    filterRdd = mapRdd.filter(lambda x:x>5)\n",
    "    print(filterRdd.collect())\n",
    "\n",
    "    print(sc.parallelize(data).map(lambda x:x*2).filter(lambda x:x>5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10]\n",
      "[6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "my_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'spark', 'hello', 'world', 'hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "def my_flatMap():\n",
    "    data = [\"hello spark\",\"hello world\",\"hello world\"]\n",
    "    rdd = sc.parallelize(data)\n",
    "    print(rdd.flatMap(lambda line:line.split(\" \")).collect())\n",
    "my_flatMap()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'spark'], ['hello', 'world'], ['hello', 'world']]\n"
     ]
    }
   ],
   "source": [
    "def my_flatMap():\n",
    "    data = [\"hello spark\",\"hello world\",\"hello world\"]\n",
    "    rdd = sc.parallelize(data)\n",
    "    print(rdd.map(lambda line:line.split(\" \")).collect())\n",
    "my_flatMap()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def my_groupBy():\n",
    "        data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "        rdd = sc.parallelize(data)\n",
    "        mapRdd = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1))\n",
    "        groupByRdd = mapRdd.groupByKey()\n",
    "        print(groupByRdd.collect())\n",
    "        print(groupByRdd.map(lambda x:{x[0]:list(x[1])}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('world', <pyspark.resultiterable.ResultIterable object at 0x7fbe80974160>), ('hello', <pyspark.resultiterable.ResultIterable object at 0x7fbe80974e10>), ('spark', <pyspark.resultiterable.ResultIterable object at 0x7fbe80974198>)]\n",
      "[{'world': [1, 1]}, {'hello': [1, 1, 1]}, {'spark': [1]}]\n"
     ]
    }
   ],
   "source": [
    "my_groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reduceByKey():\n",
    "    data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "    rdd = sc.parallelize(data)\n",
    "    mapRdd = rdd.flatMap(lambda line: line.split(\" \")).map(lambda x: (x, 1))\n",
    "    reduceByKeyRdd = mapRdd.reduceByKey(lambda a,b:a+b)\n",
    "    return reduceByKeyRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('world', 2), ('hello', 3), ('spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(my_reduceByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def my_sort():\n",
    "        data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "        rdd = sc.parallelize(data)\n",
    "        mapRdd = rdd.flatMap(lambda line: line.split(\" \")).map(lambda x: (x, 1))\n",
    "        reduceByKeyRdd = mapRdd.reduceByKey(lambda a, b: a + b)\n",
    "        reduceByKeyRdd.sortByKey(False).collect()\n",
    "\n",
    "        reduceByKeyRdd.map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    " def my_union():\n",
    "        a = sc.parallelize([1,2,3])\n",
    "        b = sc.parallelize([3,4,5])\n",
    "        a.union(b).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "def my_distinct():\n",
    "    a = sc.parallelize([1, 2, 3])\n",
    "    b = sc.parallelize([3, 4, 2])\n",
    "    print(a.union(b).distinct().collect())\n",
    "my_distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def my_join():\n",
    "        a = sc.parallelize([(\"A\", \"a1\"), (\"C\", \"c1\"), (\"D\", \"d1\"), (\"F\", \"f1\"), (\"F\", \"f2\")])\n",
    "        b = sc.parallelize([(\"A\", \"a2\"), (\"C\", \"c2\"), (\"C\", \"c3\"), (\"E\", \"e1\")])\n",
    "\n",
    "        a.fullOuterJoin(b).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
