{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T09:19:16.646051Z",
     "start_time": "2021-02-20T09:19:13.775195Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Layer\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T09:20:47.349495Z",
     "start_time": "2021-02-20T09:20:47.336891Z"
    }
   },
   "outputs": [],
   "source": [
    "class MF_layer(Layer):\n",
    "    def __init__(self, user_num, item_num, latent_dim, use_bias=False, user_reg=1e-4, item_reg=1e-4,\n",
    "                 user_bias_reg=1e-4, item_bias_reg=1e-4):\n",
    "        \"\"\"\n",
    "        MF Layer\n",
    "        :param user_num: user length\n",
    "        :param item_num: item length\n",
    "        :param latent_dim: latent number\n",
    "        :param use_bias: whether using bias or not\n",
    "        :param user_reg: regularization of user\n",
    "        :param item_reg: regularization of item\n",
    "        :param user_bias_reg: regularization of user bias\n",
    "        :param item_bias_reg: regularization of item bias\n",
    "        \"\"\"\n",
    "        super(MF_layer, self).__init__()\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.latent_dim = latent_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.user_reg = user_reg\n",
    "        self.item_reg = item_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.p = self.add_weight(name='user_latent_matrix',\n",
    "                                 shape=(self.user_num, self.latent_dim),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 regularizer=l2(self.user_reg),\n",
    "                                 trainable=True)\n",
    "        self.q = self.add_weight(name='item_latent_matrix',\n",
    "                                 shape=(self.item_num, self.latent_dim),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 regularizer=l2(self.item_reg),\n",
    "                                 trainable=True)\n",
    "        self.user_bias = self.add_weight(name='user_bias',\n",
    "                                         shape=(self.user_num, 1),\n",
    "                                         initializer=tf.random_normal_initializer(),\n",
    "                                         regularizer=l2(self.user_bias_reg),\n",
    "                                         trainable=self.use_bias)\n",
    "        self.item_bias = self.add_weight(name='item_bias',\n",
    "                                         shape=(self.item_num, 1),\n",
    "                                         initializer=tf.random_normal_initializer(),\n",
    "                                         regularizer=l2(self.item_bias_reg),\n",
    "                                         trainable=self.use_bias)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        user_id, item_id, avg_score = inputs\n",
    "        # MF\n",
    "        latent_user = tf.nn.embedding_lookup(params=self.p, ids=user_id)\n",
    "        latent_item = tf.nn.embedding_lookup(params=self.q, ids=item_id)\n",
    "        outputs = tf.reduce_sum(tf.multiply(latent_user, latent_item), axis=1, keepdims=True)\n",
    "        # MF-bias\n",
    "        user_bias = tf.nn.embedding_lookup(params=self.user_bias, ids=user_id)\n",
    "        item_bias = tf.nn.embedding_lookup(params=self.item_bias, ids=item_id)\n",
    "        bias = tf.reshape((avg_score + user_bias + item_bias), shape=(-1, 1))\n",
    "        # use bias\n",
    "        outputs = bias + outputs if self.use_bias else outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T11:35:43.868995Z",
     "start_time": "2021-02-20T11:35:43.855944Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "# use bias\n",
    "use_bias = True\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "user_reg=1e-4\n",
    "item_reg=1e-4\n",
    "\n",
    "user_bias_reg=1e-4\n",
    "item_bias_reg=1e-4\n",
    "\n",
    "num_users = 30000\n",
    "num_items = 30000\n",
    "mf_layer = MF_layer(num_users, num_items, latent_dim, use_bias,\n",
    " user_reg, item_reg, user_bias_reg, item_bias_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T11:57:58.684587Z",
     "start_time": "2021-02-20T11:57:58.662245Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sparseFeature(feat, feat_num, embed_dim=4):\n",
    "    \"\"\"\n",
    "    create dictionary for sparse feature\n",
    "    :param feat: feature name\n",
    "    :param feat_num: the total number of sparse features that do not repeat\n",
    "    :param embed_dim: embedding dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return {'feat': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}\n",
    "\n",
    "\n",
    "def denseFeature(feat):\n",
    "    \"\"\"\n",
    "    create dictionary for dense feature\n",
    "    :param feat: dense feature name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return {'feat': feat}\n",
    "\n",
    "\n",
    "def create_explicit_ml_1m_dataset(file, latent_dim=4, test_size=0.2):\n",
    "    \"\"\"\n",
    "    create the explicit dataset of movielens-1m\n",
    "    We took the last 20% of each user sorted by timestamp as the test dataset\n",
    "    Each of these samples contains UserId, MovieId, Rating, avg_score\n",
    "    :param file: dataset path\n",
    "    :param latent_dim: latent factor\n",
    "    :param test_size: ratio of test dataset\n",
    "    :return: user_num, item_num, train_df, test_df\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(file, sep=\",\", header=0,engine='python',\n",
    "                     names=['UserId', 'MovieId', 'Rating', 'Timestamp'])\n",
    "    data_df[\"UserId\"].astype('int')\n",
    "    data_df['avg_score'] = data_df.groupby(by='UserId')['Rating'].transform('mean')\n",
    "    # feature columns\n",
    "    user_num, item_num = data_df['UserId'].max() + 1, data_df['MovieId'].max() + 1\n",
    "    feature_columns = [[denseFeature('avg_score')],\n",
    "                       [sparseFeature('user_id', user_num, latent_dim),\n",
    "                        sparseFeature('item_id', item_num, latent_dim)]]\n",
    "    # split train dataset and test dataset\n",
    "    watch_count = data_df.groupby(by='UserId')['MovieId'].agg('count')\n",
    "    test_df = pd.concat([\n",
    "        data_df[data_df.UserId == i].iloc[int(0.8 * watch_count[i]):] for i in tqdm(watch_count.index)], axis=0)\n",
    "    test_df = test_df.reset_index()\n",
    "    train_df = data_df.drop(labels=test_df['index'])\n",
    "    train_df = train_df.drop(['Timestamp'], axis=1).sample(frac=1.).reset_index(drop=True)\n",
    "    test_df = test_df.drop(['index', 'Timestamp'], axis=1).sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "    train_X = [train_df['avg_score'].values, train_df[['UserId', 'MovieId']].values]\n",
    "    train_y = train_df['Rating'].values.astype('int32')\n",
    "    test_X = [test_df['avg_score'].values, test_df[['UserId', 'MovieId']].values]\n",
    "    test_y = test_df['Rating'].values.astype('int32')\n",
    "    return feature_columns, (train_X, train_y), (test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:00:28.297792Z",
     "start_time": "2021-02-20T11:59:04.097634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29776/29776 [01:02<00:00, 475.01it/s]\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "feature_columns, train, test = create_explicit_ml_1m_dataset(\"/Users/hui/Desktop/python/recommend/huichuanRecSys/spark_test/resources/ratings.csv\",latent_dim, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:02:29.007059Z",
     "start_time": "2021-02-20T12:02:29.003920Z"
    }
   },
   "outputs": [],
   "source": [
    "mf_layer = MF_layer(30001, 1001, latent_dim, use_bias,\n",
    "                                 user_reg, item_reg, user_bias_reg, item_bias_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:06:28.723321Z",
     "start_time": "2021-02-20T12:06:28.720834Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = train\n",
    "test_X, test_y = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:19:17.590567Z",
     "start_time": "2021-02-20T12:19:17.588112Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_feature_columns,sparse_feature_columns = feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:19:43.324435Z",
     "start_time": "2021-02-20T12:19:43.318535Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_inputs = tf.keras.Input(shape=(len(dense_feature_columns),), dtype=tf.float32)\n",
    "sparse_inputs = tf.keras.Input(shape=(len(sparse_feature_columns),), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:21:15.264210Z",
     "start_time": "2021-02-20T12:21:15.037877Z"
    }
   },
   "outputs": [],
   "source": [
    "user_id, item_id = sparse_inputs[:, 0], sparse_inputs[:, 1]\n",
    "avg_score = dense_inputs\n",
    "outputs = mf_layer([user_id, item_id, avg_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:21:45.206544Z",
     "start_time": "2021-02-20T12:21:45.201654Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model([dense_inputs,sparse_inputs], outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:21:52.011737Z",
     "start_time": "2021-02-20T12:21:52.007153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None,)]            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None,)]            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mf_layer_2 (MF_layer)           (None, 1)            1023066     tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,023,066\n",
      "Trainable params: 1,023,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T12:31:21.046390Z",
     "start_time": "2021-02-20T12:27:47.548598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 830601 samples, validate on 92289 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hui/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/hui/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830601/830601 [==============================] - 19s 23us/sample - loss: 0.8099 - mse: 0.7785 - val_loss: 0.8164 - val_mse: 0.7821\n",
      "Epoch 2/10\n",
      "830601/830601 [==============================] - 18s 22us/sample - loss: 0.8091 - mse: 0.7711 - val_loss: 0.8177 - val_mse: 0.7797\n",
      "Epoch 3/10\n",
      "830601/830601 [==============================] - 19s 23us/sample - loss: 0.8086 - mse: 0.7680 - val_loss: 0.8179 - val_mse: 0.7778\n",
      "Epoch 4/10\n",
      "830601/830601 [==============================] - 20s 24us/sample - loss: 0.8082 - mse: 0.7661 - val_loss: 0.8179 - val_mse: 0.7771\n",
      "Epoch 5/10\n",
      "830601/830601 [==============================] - 18s 21us/sample - loss: 0.8077 - mse: 0.7647 - val_loss: 0.8178 - val_mse: 0.7764\n",
      "Epoch 6/10\n",
      "830601/830601 [==============================] - 18s 22us/sample - loss: 0.8072 - mse: 0.7639 - val_loss: 0.8176 - val_mse: 0.7756\n",
      "Epoch 7/10\n",
      "830601/830601 [==============================] - 18s 22us/sample - loss: 0.8068 - mse: 0.7629 - val_loss: 0.8178 - val_mse: 0.7752\n",
      "Epoch 8/10\n",
      "830601/830601 [==============================] - 19s 22us/sample - loss: 0.8064 - mse: 0.7621 - val_loss: 0.8176 - val_mse: 0.7747\n",
      "Epoch 9/10\n",
      "830601/830601 [==============================] - 19s 23us/sample - loss: 0.8062 - mse: 0.7614 - val_loss: 0.8176 - val_mse: 0.7741\n",
      "Epoch 10/10\n",
      "830601/830601 [==============================] - 18s 21us/sample - loss: 0.8059 - mse: 0.7606 - val_loss: 0.8177 - val_mse: 0.7738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse: 0.905791\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import numpy as np\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate),\n",
    "                metrics=['mse'])\n",
    "# ==============================Fit==============================\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    # callbacks=[checkpoint],\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test rmse: %f' % np.sqrt(model.evaluate(test_X, test_y)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
